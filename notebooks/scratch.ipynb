{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95889a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "import whisper\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import torch\n",
    "import youtube_dl\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = os.path.expanduser('~')\n",
    "COBRA_DIR = os.path.join(HOME_DIR, 'cobra')\n",
    "DATA_DIR = os.path.join(COBRA_DIR, 'data')\n",
    "if not os.path.exists(DATA_DIR):\n",
    "  os.makedirs(DATA_DIR)\n",
    "  \n",
    "MAX_NUM_SEGMENTS = 100\n",
    "\n",
    "YDL_OPTS = {\n",
    "    'download_archive': os.path.join(DATA_DIR, 'archive.txt'),\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': os.path.join(DATA_DIR, '%(title)s.%(ext)s'),\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "        }],\n",
    "}\n",
    "\n",
    "llm = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a63b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, ydl_opts):\n",
    "  with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "      result = ydl.extract_info(\"{}\".format(url))\n",
    "      fname = ydl.prepare_filename(result)\n",
    "      return fname\n",
    "    \n",
    "def transcribe(audio_path, transcript_path):\n",
    "  if os.path.exists(transcript_path):\n",
    "    with open(transcript_path, 'r') as f:\n",
    "      result = json.load(f)\n",
    "  else:\n",
    "    whisper_model = whisper.load_model(\"base\")\n",
    "    result = whisper_model.transcribe(audio_path)\n",
    "    with open(transcript_path, 'w') as f:\n",
    "      json.dump(result, f)\n",
    "  return result['segments']\n",
    "\n",
    "def compute_seg_durations(segments):\n",
    "  return [s['end']-s['start'] for s in segments]\n",
    "\n",
    "def compute_info_densities(segments, seg_durations, llm, tokenizer, device, ctxt_len=512, verbose=False):\n",
    "  seg_encodings = [tokenizer(seg['text'], return_tensors='pt') for seg in segments]\n",
    "  input_ids = [enc.input_ids.to(device) for enc in seg_encodings]\n",
    "  seg_lens = [x.shape[1] for x in input_ids]\n",
    "  cat_input_ids = torch.cat(input_ids, axis=1)\n",
    "  end = 0\n",
    "  seg_nlls = []\n",
    "  n = cat_input_ids.shape[1]\n",
    "  for i, seg_len in enumerate(seg_lens):\n",
    "    end = min(n, end + seg_len)\n",
    "    start = max(0, end - ctxt_len)\n",
    "    ctxt_ids = cat_input_ids[:, start:end]\n",
    "    target_ids = ctxt_ids.clone()\n",
    "    target_ids[:, :-seg_len] = -100\n",
    "    avg_nll = llm(ctxt_ids, labels=target_ids).loss.detach().numpy()\n",
    "    nll = avg_nll * seg_len\n",
    "    seg_nlls.append(nll)\n",
    "    if verbose:\n",
    "      print(nll, avg_nll, i, len(seg_lens))\n",
    "  seg_nlls = np.array(seg_nlls)\n",
    "  info_densities = seg_nlls / seg_durations\n",
    "  return info_densities\n",
    "\n",
    "def smooth_info_densities(info_densities, seg_durations, max_leaf_nodes, min_sec_leaf):\n",
    "  min_samples_leaf = int(min_sec_leaf / np.mean(seg_durations))\n",
    "  tree = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, min_samples_leaf=min_samples_leaf)\n",
    "  X = np.arange(0, len(info_densities), 1)[:, np.newaxis]\n",
    "  tree.fit(X, info_densities)\n",
    "  smoothed_info_densities = tree.predict(X)\n",
    "  return smoothed_info_densities\n",
    "\n",
    "def squash_segs(segments, info_densities):\n",
    "  start = segments[0]['start']\n",
    "  end = None\n",
    "  seg_times = []\n",
    "  seg_densities = [info_densities[0]]\n",
    "  for i in range(1, len(segments)):\n",
    "    curr_density = info_densities[i]\n",
    "    if curr_density != info_densities[i-1]:\n",
    "      seg = segments[i]\n",
    "      seg_start = seg['start']\n",
    "      seg_times.append((start, seg_start))\n",
    "      seg_densities.append(curr_density)\n",
    "      start = seg_start\n",
    "  seg_times.append((start, seg['end']))\n",
    "  return seg_times, seg_densities\n",
    "\n",
    "def compute_speedups(info_densities):\n",
    "  avg_density = np.mean(info_densities)\n",
    "  speedups = avg_density / info_densities\n",
    "  return speedups\n",
    "\n",
    "def compute_actual_speedup(durations, speedups, total_duration):\n",
    "  spedup_durations = durations / speedups\n",
    "  spedup_total_duration = spedup_durations.sum()\n",
    "  actual_speedup_factor = total_duration/spedup_total_duration\n",
    "  return spedup_total_duration, actual_speedup_factor\n",
    "\n",
    "def postprocess_speedups(speedups, factor, min_speedup, max_speedup, durations, total_duration, thresh=0.01):\n",
    "  assert min_speedup <= factor and factor <= max_speedup\n",
    "  tuned_factor = np.array([factor/10, factor*10])\n",
    "  actual_speedup_factor = None\n",
    "  while actual_speedup_factor is None or abs(actual_speedup_factor - factor) / factor > thresh:\n",
    "    mid = tuned_factor.mean()\n",
    "    tuned_speedups = speedups * mid\n",
    "    tuned_speedups = np.round(tuned_speedups, decimals=2)\n",
    "    tuned_speedups = np.clip(tuned_speedups, min_speedup, max_speedup)\n",
    "    _, actual_speedup_factor = compute_actual_speedup(durations, tuned_speedups, total_duration)\n",
    "    tuned_factor[0 if actual_speedup_factor < factor else 1] = mid\n",
    "  return tuned_speedups\n",
    "\n",
    "def cat_clips(seg_times, speedups, audio_path, output_path):\n",
    "  if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "  in_file = ffmpeg.input(audio_path)\n",
    "  segs = []\n",
    "  for (start, end), speedup in zip(seg_times, speedups):\n",
    "    seg = in_file.filter('atrim', start=start, end=end).filter('atempo', speedup)\n",
    "    segs.append(seg)\n",
    "  cat = ffmpeg.concat(*segs, v=0, a=1)\n",
    "  cat.output(output_path).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.youtube.com/watch?v=03p2CADwGF8'\n",
    "speedup_factor = 1.5\n",
    "min_speedup = 1\n",
    "max_speedup = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36090dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_speedup = max(0.5, min_speedup) # ffmpeg limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = download(url, YDL_OPTS)\n",
    "assert name.endswith('.m4a')\n",
    "name = name.split('.m4a')[0].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69227776",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = os.path.join(DATA_DIR, '%s.mp3' % name)\n",
    "transcript_path = os.path.join(DATA_DIR, '%s.json' % name)\n",
    "output_path = os.path.join(DATA_DIR, '%s_smooth.mp3' % name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb7e51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segments = transcribe(audio_path, transcript_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_durations = compute_seg_durations(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c3ed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info_densities = compute_info_densities(\n",
    "  segments, seg_durations, llm, tokenizer, device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971bb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = segments[-1]['end'] - segments[0]['start']\n",
    "min_sec_leaf = total_duration / MAX_NUM_SEGMENTS\n",
    "smoothed_info_densities = smooth_info_densities(\n",
    "  info_densities, \n",
    "  seg_durations,\n",
    "  MAX_NUM_SEGMENTS,\n",
    "  min_sec_leaf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a815a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "squashed_times, squashed_densities = squash_segs(segments, smoothed_info_densities)\n",
    "squashed_durations = np.array([end-start for start, end in squashed_times])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6efea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "speedups = compute_speedups(squashed_densities)\n",
    "speedups = postprocess_speedups(speedups, speedup_factor, min_speedup, max_speedup, squashed_durations, total_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_clips(squashed_times, speedups, audio_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spedup_total_duration, actual_speedup_factor = compute_actual_speedup(squashed_durations, speedups, total_duration)\n",
    "print('output: %s' % output_path)\n",
    "print('new length: %d minutes' % (spedup_total_duration/60))\n",
    "print('speedup factor: %0.1f' % actual_speedup_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b072022",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Time (minutes)')\n",
    "plt.ylabel('Information density (bits per second)')\n",
    "times = np.array([seg['start'] for seg in segments])\n",
    "plt.plot(times/60, info_densities/np.log(2), color='gray')\n",
    "plt.step(times/60, smoothed_info_densities/np.log(2), color='orange')\n",
    "plt.savefig(os.path.join(DATA_DIR, '%s_info.pdf' % name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d629266",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_starts, _ = list(zip(*squashed_times))\n",
    "plt.xlabel('Time (minutes)')\n",
    "plt.ylabel('Speedup')\n",
    "plt.step(np.array(seg_starts)/60, speedups, color='orange')\n",
    "plt.savefig(os.path.join(DATA_DIR, '%s_speedup.pdf' % name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5a5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
